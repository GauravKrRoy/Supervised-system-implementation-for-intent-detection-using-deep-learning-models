{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, pickle, sys, json, random, math \n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, add, concatenate, TimeDistributed, Bidirectional\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from keras.layers import Dense, Input, Flatten, Merge, Dropout, concatenate, Concatenate, merge\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/gauravroy/Desktop/intent/embedding_matrix.pickle', 'rb') as fp:\n",
    "    embedding_matrix = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/train_sequence.pickle', 'rb') as fp:\n",
    "    train_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/test_sequence.pickle', 'rb') as fp:\n",
    "    test_sequence = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/intent_slot_word_dict.pickle', 'rb') as fp:\n",
    "    intent_slot_word_dict = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/train_idata.pickle', 'rb') as fp:\n",
    "    itrain = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/test_idata.pickle', 'rb') as fp:\n",
    "    itest = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/y_train_intent.pickle', 'rb') as fp:\n",
    "    y_train_intent = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "with open('/home/gauravroy/Desktop/intent/y_test_intent.pickle', 'rb') as fp:\n",
    "    y_test_intent = pickle.load(fp)\n",
    "fp.close()\n",
    "\n",
    "intent_dict, words_dict = intent_slot_word_dict\n",
    "\n",
    "max_len = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_intent = list()\n",
    "z_test_intent = list()\n",
    "\n",
    "for out in y_train_intent:\n",
    "    for i in range(len(out)):\n",
    "        if(out[i] == 1):\n",
    "            z_train_intent.append(i)\n",
    "        \n",
    "for out in y_test_intent:\n",
    "    for i in range(len(out)):\n",
    "        if(out[i] == 1):\n",
    "            z_test_intent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 10,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 7,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 2,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 2,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 0,\n",
       " 6,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_test_intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(list1, list2):\n",
    "    if(len(list1) != len(list2)):\n",
    "        print(\"Size of a the lists not equal\")\n",
    "        return\n",
    "    count = 0.0\n",
    "    for i in range(len(list1)):\n",
    "        if(list1[i] == list2[i]):\n",
    "            count = count + 1\n",
    "            \n",
    "    return count/len(list1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 CNN With Different Kernel Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=20)`\n"
     ]
    }
   ],
   "source": [
    "cnn_embedding_layer = Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "cnn_embedded_sequences = cnn_embedding_layer(sequence_input)\n",
    "\n",
    "conv0 = Conv1D(filters=100, kernel_size=2, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "flatten0 = Flatten()(pool0)\n",
    "\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool1 = MaxPooling1D(2)(conv1)\n",
    "flatten1 = Flatten()(pool1)\n",
    "                                                # identity and check\n",
    "conv2 = Conv1D(filters=100, kernel_size=4, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool2 = MaxPooling1D(2)(conv2)\n",
    "flatten2 = Flatten()(pool2)\n",
    "\n",
    "# out = Merge(mode='concat')([flatten0,flatten1,flatten2])\n",
    "out = concatenate([flatten0,flatten1,flatten2])\n",
    "\n",
    "graph = Model(inputs=sequence_input, outputs=out)\n",
    "\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(graph)\n",
    "cnn_model.add(Dropout(0.10))\n",
    "cnn_model.add(Dense(output_dim=20, activation='relu'))\n",
    "cnn_model.add(Dense(units=len(intent_dict), activation='softmax',kernel_initializer='he_normal'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 47)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 47, 600)      514800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 46, 100)      120100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 45, 100)      180100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 44, 100)      240100      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 23, 100)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 22, 100)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 22, 100)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2300)         0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2200)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2200)         0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6700)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,055,100\n",
      "Trainable params: 540,300\n",
      "Non-trainable params: 514,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "graph.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5355 samples, validate on 1336 samples\n",
      "Epoch 1/10\n",
      " - 13s - loss: 1.4002 - acc: 0.5785 - categorical_accuracy: 0.5785 - val_loss: 1.0910 - val_acc: 0.7036 - val_categorical_accuracy: 0.7036\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70359, saving model to cnn_weights.hdf5\n",
      "Epoch 2/10\n",
      " - 11s - loss: 0.8700 - acc: 0.7374 - categorical_accuracy: 0.7374 - val_loss: 0.9832 - val_acc: 0.7163 - val_categorical_accuracy: 0.7163\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70359 to 0.71632, saving model to cnn_weights.hdf5\n",
      "Epoch 3/10\n",
      " - 11s - loss: 0.7493 - acc: 0.7677 - categorical_accuracy: 0.7677 - val_loss: 0.9218 - val_acc: 0.7433 - val_categorical_accuracy: 0.7433\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.71632 to 0.74326, saving model to cnn_weights.hdf5\n",
      "Epoch 4/10\n",
      " - 12s - loss: 0.6602 - acc: 0.7955 - categorical_accuracy: 0.7955 - val_loss: 0.9165 - val_acc: 0.7320 - val_categorical_accuracy: 0.7320\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.74326\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.5866 - acc: 0.8133 - categorical_accuracy: 0.8133 - val_loss: 0.9874 - val_acc: 0.7373 - val_categorical_accuracy: 0.7373\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.74326\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.5240 - acc: 0.8364 - categorical_accuracy: 0.8364 - val_loss: 0.9441 - val_acc: 0.7403 - val_categorical_accuracy: 0.7403\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.74326\n",
      "Epoch 7/10\n",
      " - 11s - loss: 0.4571 - acc: 0.8575 - categorical_accuracy: 0.8575 - val_loss: 0.9148 - val_acc: 0.7380 - val_categorical_accuracy: 0.7380\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.74326\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.4049 - acc: 0.8730 - categorical_accuracy: 0.8730 - val_loss: 0.9896 - val_acc: 0.7418 - val_categorical_accuracy: 0.7418\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.74326\n",
      "Epoch 9/10\n",
      " - 11s - loss: 0.3686 - acc: 0.8827 - categorical_accuracy: 0.8827 - val_loss: 0.9937 - val_acc: 0.7335 - val_categorical_accuracy: 0.7335\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.74326\n",
      "Epoch 10/10\n",
      " - 12s - loss: 0.3322 - acc: 0.8952 - categorical_accuracy: 0.8952 - val_loss: 1.0470 - val_acc: 0.7343 - val_categorical_accuracy: 0.7343\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.74326\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "#              optimizer='sgd' optimizer='adam',metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "\n",
    "saveBestModel = ModelCheckpoint(filepath=\"cnn_weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "cnn_model.fit(train_sequence, np.array(y_train_intent),validation_data=(test_sequence,np.array(y_test_intent)), epochs=10, batch_size=50, verbose=2, callbacks=[saveBestModel])\n",
    "cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_classes_train = cnn_model.predict_classes(train_sequence)\n",
    "cnn_pred_classes_test = cnn_model.predict_classes(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5355,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(cnn_pred_classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5355, 47)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 88.12324929971989%\n",
      "Test Accuracy: 75.0%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(z_train_intent,cnn_pred_classes_train)*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(z_test_intent,cnn_pred_classes_test)*100) + \"%\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 CNN with LSTM and Same Kernel_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_embedding_layer = Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "cnn_embedded_sequences = cnn_embedding_layer(sequence_input)\n",
    "\n",
    "conv0 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "flatten0 = Flatten()(pool0)\n",
    "\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool1 = MaxPooling1D(2)(conv1)\n",
    "flatten1 = Flatten()(pool1)\n",
    "                                                # identity and check\n",
    "conv2 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool2 = MaxPooling1D(2)(conv2)\n",
    "#pool2  = pool2.get_shape()\n",
    "flatten2 = Flatten()(pool2)\n",
    "\n",
    "# out = Merge(mode='concat')([flatten0,flatten1,flatten2])\n",
    "out = concatenate([pool0,pool1,pool2])\n",
    "\n",
    "lstm = LSTM(200,return_sequences=False)(out)\n",
    "\n",
    "out = Dense(units=200, activation='relu', kernel_initializer='he_normal')(lstm)\n",
    "\n",
    "out = Dense(units=len(intent_dict), activation='softmax', kernel_initializer='he_normal')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=20)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "graph = Model(inputs=sequence_input, outputs=out)\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(graph)\n",
    "#cnn_model.add(get_shape(47*2))\n",
    "#cnn_model.add(get_shape(None,22,100))\n",
    "cnn_model.add(Dropout(0.10))\n",
    "cnn_model.add(Dense(output_dim=20, activation='relu'))\n",
    "cnn_model.add(Dense(units=len(intent_dict), activation='softmax',kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 47)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 47, 600)      514800      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 45, 100)      180100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 45, 100)      180100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 45, 100)      180100      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 22, 100)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 22, 100)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 22, 100)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 22, 300)      0           max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "                                                                 max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 200)          400800      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 200)          40200       lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           2412        dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,498,512\n",
      "Trainable params: 983,712\n",
      "Non-trainable params: 514,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "graph.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5355 samples, validate on 1336 samples\n",
      "Epoch 1/10\n",
      " - 19s - loss: 2.1992 - acc: 0.3302 - categorical_accuracy: 0.3302 - val_loss: 1.9662 - val_acc: 0.4499 - val_categorical_accuracy: 0.4499\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.44985, saving model to cnn_weights.hdf5\n",
      "Epoch 2/10\n",
      " - 21s - loss: 1.8064 - acc: 0.3864 - categorical_accuracy: 0.3864 - val_loss: 1.6951 - val_acc: 0.4626 - val_categorical_accuracy: 0.4626\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.44985 to 0.46257, saving model to cnn_weights.hdf5\n",
      "Epoch 3/10\n",
      " - 22s - loss: 1.6801 - acc: 0.3798 - categorical_accuracy: 0.3798 - val_loss: 1.6388 - val_acc: 0.4626 - val_categorical_accuracy: 0.4626\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.46257 to 0.46257, saving model to cnn_weights.hdf5\n",
      "Epoch 4/10\n",
      " - 22s - loss: 1.5860 - acc: 0.4069 - categorical_accuracy: 0.4069 - val_loss: 1.5193 - val_acc: 0.5277 - val_categorical_accuracy: 0.5277\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.46257 to 0.52769, saving model to cnn_weights.hdf5\n",
      "Epoch 5/10\n",
      " - 23s - loss: 1.4966 - acc: 0.4319 - categorical_accuracy: 0.4319 - val_loss: 1.4883 - val_acc: 0.5037 - val_categorical_accuracy: 0.5037\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.52769\n",
      "Epoch 6/10\n",
      " - 23s - loss: 1.4728 - acc: 0.4306 - categorical_accuracy: 0.4306 - val_loss: 1.4285 - val_acc: 0.5389 - val_categorical_accuracy: 0.5389\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.52769 to 0.53892, saving model to cnn_weights.hdf5\n",
      "Epoch 7/10\n",
      " - 22s - loss: 1.4295 - acc: 0.4506 - categorical_accuracy: 0.4506 - val_loss: 1.4458 - val_acc: 0.5352 - val_categorical_accuracy: 0.5352\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.53892\n",
      "Epoch 8/10\n",
      " - 22s - loss: 1.3945 - acc: 0.4556 - categorical_accuracy: 0.4556 - val_loss: 1.3821 - val_acc: 0.5374 - val_categorical_accuracy: 0.5374\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.53892\n",
      "Epoch 9/10\n",
      " - 22s - loss: 1.3676 - acc: 0.4502 - categorical_accuracy: 0.4502 - val_loss: 1.3389 - val_acc: 0.5389 - val_categorical_accuracy: 0.5389\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.53892\n",
      "Epoch 10/10\n",
      " - 23s - loss: 1.3289 - acc: 0.5072 - categorical_accuracy: 0.5072 - val_loss: 1.3359 - val_acc: 0.5838 - val_categorical_accuracy: 0.5838\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.53892 to 0.58383, saving model to cnn_weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff24919e80>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "#              optimizer='sgd' optimizer='adam',metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "saveBestModel = ModelCheckpoint(filepath=\"cnn_weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "cnn_model.fit(np.array(train_sequence), np.array(y_train_intent),validation_data=(np.array(test_sequence),np.array(y_test_intent)), epochs=10, batch_size=50, verbose=2, callbacks=[saveBestModel])\n",
    "#cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_classes_train = cnn_model.predict_classes(train_sequence)\n",
    "cnn_pred_classes_test = cnn_model.predict_classes(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 54.957983193277315%\n",
      "Test Accuracy: 58.38323353293413%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(z_train_intent,cnn_pred_classes_train)*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(z_test_intent,cnn_pred_classes_test)*100) + \"%\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 CNN with Bidirectional LSTM and Same Kernel_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_embedding_layer = Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "cnn_embedded_sequences = cnn_embedding_layer(sequence_input)\n",
    "\n",
    "conv0 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "flatten0 = Flatten()(pool0)\n",
    "\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool1 = MaxPooling1D(2)(conv1)\n",
    "flatten1 = Flatten()(pool1)\n",
    "                                                # identity and check\n",
    "conv2 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool2 = MaxPooling1D(2)(conv2)\n",
    "#pool2  = pool2.get_shape()\n",
    "flatten2 = Flatten()(pool2)\n",
    "\n",
    "# out = Merge(mode='concat')([flatten0,flatten1,flatten2])\n",
    "out = concatenate([pool0,pool1,pool2])\n",
    "\n",
    "lstm = Bidirectional(LSTM(200,return_sequences=False))(out)\n",
    "#lstm = tf.reshape(lstm,[400,1], name=None)\n",
    "out = Dense(units=200, activation='relu', kernel_initializer='he_normal')(lstm)\n",
    "\n",
    "out = Dense(units=len(intent_dict), activation='softmax', kernel_initializer='he_normal')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(12)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(400)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(47)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(12)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Model(inputs=sequence_input, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=20)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(graph)\n",
    "#cnn_model.add(get_shape(47*2))\n",
    "#cnn_model.add(get_shape(None,22,100))\n",
    "cnn_model.add(Dropout(0.10))\n",
    "cnn_model.add(Dense(output_dim=20, activation='relu'))\n",
    "cnn_model.add(Dense(units=len(intent_dict), activation='softmax',kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 47)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 47, 600)      514800      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 45, 100)      180100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 100)      180100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 45, 100)      180100      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 22, 100)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 22, 100)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 22, 100)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 22, 300)      0           max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 400)          801600      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          80200       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 12)           2412        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,939,312\n",
      "Trainable params: 1,424,512\n",
      "Non-trainable params: 514,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "graph.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_intent = np.array(y_train_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_intent = y_train_intent.reshape(5355,12,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(y_train_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "#              optimizer='sgd' optimizer='adam',metrics=[metrics.mae, metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel = ModelCheckpoint(filepath=\"cnn_weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5355 samples, validate on 1336 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 2.2215 - acc: 0.3175 - categorical_accuracy: 0.3175 - val_loss: 1.9331 - val_acc: 0.5344 - val_categorical_accuracy: 0.5344\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53443, saving model to cnn_weights.hdf5\n",
      "Epoch 2/10\n",
      " - 33s - loss: 1.7638 - acc: 0.4726 - categorical_accuracy: 0.4726 - val_loss: 1.5768 - val_acc: 0.5696 - val_categorical_accuracy: 0.5696\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.53443 to 0.56961, saving model to cnn_weights.hdf5\n",
      "Epoch 3/10\n",
      " - 32s - loss: 1.5441 - acc: 0.4792 - categorical_accuracy: 0.4792 - val_loss: 1.4703 - val_acc: 0.5704 - val_categorical_accuracy: 0.5704\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.56961 to 0.57036, saving model to cnn_weights.hdf5\n",
      "Epoch 4/10\n",
      " - 32s - loss: 1.4172 - acc: 0.5117 - categorical_accuracy: 0.5117 - val_loss: 1.3927 - val_acc: 0.5928 - val_categorical_accuracy: 0.5928\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.57036 to 0.59281, saving model to cnn_weights.hdf5\n",
      "Epoch 5/10\n",
      " - 34s - loss: 1.3616 - acc: 0.5283 - categorical_accuracy: 0.5283 - val_loss: 1.4069 - val_acc: 0.5591 - val_categorical_accuracy: 0.5591\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.59281\n",
      "Epoch 6/10\n",
      " - 32s - loss: 1.2826 - acc: 0.5894 - categorical_accuracy: 0.5894 - val_loss: 1.2318 - val_acc: 0.6609 - val_categorical_accuracy: 0.6609\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.59281 to 0.66093, saving model to cnn_weights.hdf5\n",
      "Epoch 7/10\n",
      " - 33s - loss: 1.1794 - acc: 0.6347 - categorical_accuracy: 0.6347 - val_loss: 1.1517 - val_acc: 0.6856 - val_categorical_accuracy: 0.6856\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.66093 to 0.68563, saving model to cnn_weights.hdf5\n",
      "Epoch 8/10\n",
      " - 34s - loss: 1.1185 - acc: 0.6484 - categorical_accuracy: 0.6484 - val_loss: 1.1115 - val_acc: 0.6901 - val_categorical_accuracy: 0.6901\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.68563 to 0.69012, saving model to cnn_weights.hdf5\n",
      "Epoch 9/10\n",
      " - 32s - loss: 1.0603 - acc: 0.6573 - categorical_accuracy: 0.6573 - val_loss: 1.0622 - val_acc: 0.7066 - val_categorical_accuracy: 0.7066\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.69012 to 0.70659, saving model to cnn_weights.hdf5\n",
      "Epoch 10/10\n",
      " - 34s - loss: 1.0073 - acc: 0.6779 - categorical_accuracy: 0.6779 - val_loss: 1.0435 - val_acc: 0.7163 - val_categorical_accuracy: 0.7163\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.70659 to 0.71632, saving model to cnn_weights.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7eff2d0b9f98>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(np.array(train_sequence), np.array(y_train_intent),validation_data=(np.array(test_sequence),np.array(y_test_intent)), epochs=10, batch_size=50, verbose=2, callbacks=[saveBestModel])\n",
    "#cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_classes_train = cnn_model.predict_classes(train_sequence)\n",
    "cnn_pred_classes_test = cnn_model.predict_classes(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 75.31279178338002%\n",
      "Test Accuracy: 71.63173652694611%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(z_train_intent,cnn_pred_classes_train)*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(z_test_intent,cnn_pred_classes_test)*100) + \"%\")                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 CNN With LSTM And Different Kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/home/gauravroy/.local/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "cnn_embedding_layer = Embedding(len(embedding_matrix), len(embedding_matrix[0]), weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
    "\n",
    "cnn_embedded_sequences = cnn_embedding_layer(sequence_input)\n",
    "\n",
    "conv0 = Conv1D(filters=100, kernel_size=2, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool0 = MaxPooling1D(2)(conv0)\n",
    "flatten0 = Flatten()(pool0)\n",
    "\n",
    "conv1 = Conv1D(filters=100, kernel_size=3, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool1 = MaxPooling1D(2)(conv1)\n",
    "flatten1 = Flatten()(pool1)\n",
    "                                                # identity and check\n",
    "conv2 = Conv1D(filters=100, kernel_size=4, activation='relu', padding='valid',kernel_initializer='he_normal')(cnn_embedded_sequences)\n",
    "pool2 = MaxPooling1D(2)(conv2)\n",
    "#pool2  = pool2.get_shape()\n",
    "flatten2 = Flatten()(pool2)\n",
    "\n",
    "# out = Merge(mode='concat')([flatten0,flatten1,flatten2])\n",
    "out = merge([pool0, pool1, pool2], mode='concat', concat_axis=1)\n",
    "#out = concatenate([pool0,pool1,pool2])\n",
    "\n",
    "lstm = LSTM(200,return_sequences=False)(out)\n",
    "#lstm = tf.reshape(lstm,[400,1], name=None)\n",
    "out = Dense(units=200, activation='relu', kernel_initializer='he_normal')(lstm)\n",
    "\n",
    "out = Dense(units=len(intent_dict), activation='softmax', kernel_initializer='he_normal')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauravroy/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=20)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "graph = Model(inputs=sequence_input, outputs=out)\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(graph)\n",
    "#cnn_model.add(get_shape(47*2))\n",
    "#cnn_model.add(get_shape(None,22,100))\n",
    "cnn_model.add(Dropout(0.10))\n",
    "cnn_model.add(Dense(output_dim=20, activation='relu'))\n",
    "cnn_model.add(Dense(units=len(intent_dict), activation='softmax',kernel_initializer='he_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5355 samples, validate on 1336 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 2.2552 - acc: 0.1908 - categorical_accuracy: 0.1908 - val_loss: 2.0478 - val_acc: 0.2994 - val_categorical_accuracy: 0.2994\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.29940, saving model to cnn_weights.hdf5\n",
      "Epoch 2/10\n",
      " - 33s - loss: 2.0555 - acc: 0.2050 - categorical_accuracy: 0.2050 - val_loss: 1.9656 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.29940\n",
      "Epoch 3/10\n",
      " - 33s - loss: 2.0053 - acc: 0.2209 - categorical_accuracy: 0.2209 - val_loss: 1.9632 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.29940\n",
      "Epoch 4/10\n",
      " - 34s - loss: 1.9903 - acc: 0.2217 - categorical_accuracy: 0.2217 - val_loss: 1.9598 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.29940\n",
      "Epoch 5/10\n",
      " - 33s - loss: 1.9811 - acc: 0.2217 - categorical_accuracy: 0.2217 - val_loss: 1.9623 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.29940\n",
      "Epoch 6/10\n",
      " - 33s - loss: 1.9786 - acc: 0.2217 - categorical_accuracy: 0.2217 - val_loss: 1.9684 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.29940\n",
      "Epoch 7/10\n",
      " - 33s - loss: 1.9748 - acc: 0.2217 - categorical_accuracy: 0.2217 - val_loss: 1.9645 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.29940\n",
      "Epoch 8/10\n",
      " - 32s - loss: 1.9737 - acc: 0.2202 - categorical_accuracy: 0.2202 - val_loss: 1.9674 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.29940\n",
      "Epoch 9/10\n",
      " - 42s - loss: 1.9712 - acc: 0.2217 - categorical_accuracy: 0.2217 - val_loss: 1.9653 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.29940\n",
      "Epoch 10/10\n",
      " - 40s - loss: 1.9720 - acc: 0.2209 - categorical_accuracy: 0.2209 - val_loss: 1.9729 - val_acc: 0.1766 - val_categorical_accuracy: 0.1766\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.29940\n"
     ]
    }
   ],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy'])\n",
    "#              optimizer='sgd' optimizer='adam',metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "\n",
    "saveBestModel = ModelCheckpoint(filepath=\"cnn_weights.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "cnn_model.fit(train_sequence, np.array(y_train_intent),validation_data=(test_sequence,np.array(y_test_intent)), epochs=10, batch_size=50, verbose=2, callbacks=[saveBestModel])\n",
    "cnn_model.load_weights(\"cnn_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_classes_train = cnn_model.predict_classes(train_sequence)\n",
    "cnn_pred_classes_test = cnn_model.predict_classes(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 20.11204481792717%\n",
      "Test Accuracy: 29.94011976047904%\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Accuracy: \" + str(accu(z_train_intent,cnn_pred_classes_train)*100) + \"%\")\n",
    "print (\"Test Accuracy: \" + str(accu(z_test_intent,cnn_pred_classes_test)*100) + \"%\")                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
